{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Your Algorithm\n",
    "\n",
    "## Instructions\n",
    "1. From the **Pulse Rate Algorithm** Notebook you can do one of the following:\n",
    "   - Copy over all the **Code** section to the following Code block.\n",
    "   - Download as a Python (`.py`) and copy the code to the following Code block.\n",
    "2. In the bottom right, click the <span style=\"color:blue\">Test Run</span> button. \n",
    "\n",
    "### Didn't Pass\n",
    "If your code didn't pass the test, go back to the previous Concept or to your local setup and continue iterating on your algorithm and try to bring your training error down before testing again.\n",
    "\n",
    "### Pass\n",
    "If your code passes the test, complete the following! You **must** include a screenshot of your code and the Test being **Passed**. Here is what the starter filler code looks like when the test is run and should be similar. A passed test will include in the notebook a green outline plus a box with **Test passed:** and in the Results bar at the bottom the progress bar will be at 100% plus a checkmark with **All cells passed**.\n",
    "![Example](example.png)\n",
    "\n",
    "1. Take a screenshot of your code passing the test, make sure it is in the format `.png`. If not a `.png` image, you will have to edit the Markdown render the image after Step 3. Here is an example of what the `passed.png` would look like \n",
    "2. Upload the screenshot to the same folder or directory as this jupyter notebook.\n",
    "3. Rename the screenshot to `passed.png` and it should show up below.\n",
    "![Passed](passed.png)\n",
    "4. Download this jupyter notebook as a `.pdf` file. \n",
    "5. Continue to Part 2 of the Project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "edited": true,
    "gradable": true,
    "grader_id": "nrtnppao4pm",
    "udacity_user_query": ""
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, find_peaks, spectrogram\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def BandpassFilter(signal, Wn, fs, N=3):\n",
    "    \"\"\"Bandpass filter the signal.\n",
    "    Args:\n",
    "        signal: The signal.\n",
    "        Wn: The critial frequency or frequences.\n",
    "        fs: The sampling frequency.\n",
    "        N: The order of the filter.\n",
    "    \"\"\"\n",
    "    b, a = butter(N, Wn, btype='bandpass', fs=fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the\n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika DAT.mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def LoadTroikaRefFile(ref_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts labels from a troika ref file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        bpm = LoadTroikaRefFile(ref_fls[0])\n",
    "\n",
    "    Args:\n",
    "        ref_fl: (str) filepath to a troika REF .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy array for bpm labels.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(ref_fl)['BPM0']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability.\n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding\n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Ensure pr_errors and confidence_est have the same length\n",
    "    min_length = min(len(pr_errors), len(confidence_est))\n",
    "    pr_errors = pr_errors[:min_length]\n",
    "    confidence_est = confidence_est[:min_length]\n",
    "\n",
    "    # Now apply the boolean index\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confs = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "\n",
    "    # Compute aggregate error metric\n",
    "    try:\n",
    "        errs = np.hstack(errs)\n",
    "        confs = np.hstack(confs)\n",
    "    except ValueError:\n",
    "        # Ensure all arrays have the same size along dimension 0\n",
    "        min_size = min(arr.shape[0] for arr in errs)\n",
    "        errs = [arr[:min_size] for arr in errs]\n",
    "\n",
    "        # Now concatenate the arrays\n",
    "        errs = np.hstack(errs)\n",
    "        confs = np.hstack(confs)\n",
    "\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "\n",
    "def CreateSpectogram(signal, fs=125, nfft=1000):\n",
    "    \"\"\"\n",
    "    Create spectrogram from signal.\n",
    "\n",
    "    Args:\n",
    "        signal: Input signal (1D numpy array)\n",
    "        fs: Sampling frequency (Hz)\n",
    "        nfft: Number of FFT bins\n",
    "\n",
    "    Returns:\n",
    "        freqs: Frequency array (Hz)\n",
    "        times: Time array (s)\n",
    "        power: Power spectral density (1D array)\n",
    "        spec: Spectrogram (2D array of power values)\n",
    "    \"\"\"\n",
    "    # Calculate spectrogram\n",
    "    freqs, times, spec = spectrogram(\n",
    "        signal,\n",
    "        fs=fs,\n",
    "        window='triang',\n",
    "        nfft=nfft,\n",
    "        detrend='constant',\n",
    "        scaling='density',\n",
    "        mode='magnitude'\n",
    "    )\n",
    "\n",
    "    # Average power over time dimension\n",
    "    power = np.mean(spec, axis=1)\n",
    "\n",
    "    return freqs, times, power, spec\n",
    "\n",
    "\n",
    "def FindDominantFrequency(freqs, power, min_freq=60, max_freq=240):\n",
    "    \"\"\"\n",
    "    Find dominant frequency in power spectrum within physiological range\n",
    "    Args:\n",
    "        freqs: Frequency array (Hz)\n",
    "        power: Power spectral density (1D or 2D array)\n",
    "        min_freq: Minimum valid frequency (Hz)\n",
    "        max_freq: Maximum valid frequency (Hz)\n",
    "    Returns:\n",
    "        dominant_freq: (float|None) Dominant frequency (Hz) or None if no valid peak\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if freqs is None or power is None:\n",
    "        return None\n",
    "\n",
    "    # Filter frequency range\n",
    "    valid_idx = np.logical_and(freqs >= min_freq, freqs <= max_freq)\n",
    "    valid_freqs = freqs[valid_idx]\n",
    "    valid_power = power[valid_idx]\n",
    "\n",
    "    if len(valid_power) == 0:\n",
    "        return None\n",
    "\n",
    "    # Find peaks\n",
    "    peaks, _ = sp.signal.find_peaks(valid_power)\n",
    "\n",
    "    if len(peaks) == 0:\n",
    "        return None\n",
    "\n",
    "    # Sort peaks by power and return strongest\n",
    "    peak_powers = valid_power[peaks]\n",
    "    sorted_idx = np.argsort(peak_powers)[::-1]\n",
    "    return valid_freqs[peaks[sorted_idx[0]]]\n",
    "\n",
    "\n",
    "def EstimatePulseRate(ppg_signal, acc_magnitude, feature_list, fs, Wn, peak_detect_threshold=0.5,\n",
    "                      ppg_quality_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Estimate pulse rate from PPG and accelerometer signals using a machine learning model.\n",
    "    Args:\n",
    "        ppg_signal: Preprocessed PPG signal (1D numpy array)\n",
    "        acc_magnitude: Preprocessed accelerometer magnitude signal (1D numpy array)\n",
    "        feature_list: Features of the window\n",
    "        fs: Sampling frequency\n",
    "        Wn: Critical frequencies for bandpass filter\n",
    "        peak_detect_threshold: Minimum distance between peaks (s)\n",
    "        ppg_quality_threshold: Minimum signal quality index for PPG\n",
    "    Returns:\n",
    "        Estimated pulse rate in BPM\n",
    "    \"\"\"\n",
    "    # Bandpass filter signals\n",
    "    ppg_signal_filtered = BandpassFilter(ppg_signal, Wn, fs)\n",
    "    acc_magnitude_filtered = BandpassFilter(acc_magnitude, Wn, fs)\n",
    "\n",
    "    # Detect peaks in PPG signal\n",
    "    ppg_peaks, _ = find_peaks(ppg_signal_filtered, distance=fs * peak_detect_threshold)\n",
    "\n",
    "    # Compute pulse-to-pulse intervals\n",
    "    ppg_intervals = np.diff(ppg_peaks) / fs\n",
    "\n",
    "    # Derive heart rate from pulse-to-pulse intervals\n",
    "    hr_time = 60 / np.mean(ppg_intervals) if len(ppg_intervals) > 0 else 0\n",
    "\n",
    "    # Compute spectrogram for PPG signal\n",
    "    ppg_freqs, _, ppg_power, _ = CreateSpectogram(ppg_signal_filtered, fs)\n",
    "\n",
    "    # Compute spectrogram for accelerometer magnitude signal\n",
    "    _, _, acc_magnitude_power, _ = CreateSpectogram(acc_magnitude_filtered, fs)\n",
    "\n",
    "    # Find dominant frequency in PPG spectrum\n",
    "    ppg_freq = FindDominantFrequency(ppg_freqs, ppg_power)\n",
    "\n",
    "    # Adjust PPG frequency estimate for movement using accelerometer magnitude\n",
    "    ppg_freq = AdjustPPGEstimateForMovement(acc_magnitude_power, ppg_freq, ppg_freqs, ppg_power)\n",
    "\n",
    "    # Compute a signal quality index for the PPG\n",
    "    ppg_quality = np.sum(ppg_power[(ppg_freqs >= Wn[0]) & (ppg_freqs <= Wn[1])]) / np.sum(ppg_power)\n",
    "\n",
    "    # Use the machine learning model to predict the pulse rate\n",
    "    est_bpm_ml = model.predict([feature_list])[0]\n",
    "\n",
    "    # If motion is low and time-domain & frequency-domain agree, accept that as HR\n",
    "    if ppg_quality > ppg_quality_threshold and hr_time is not None and ppg_freq is not None:\n",
    "        return (hr_time + est_bpm_ml) / 2\n",
    "\n",
    "    return est_bpm_ml\n",
    "\n",
    "\n",
    "def AdjustPPGEstimateForMovement(acc_magnitude_power, ppg_freq, ppg_freqs, ppg_power):\n",
    "    \"\"\"\n",
    "    Detect movement in accelerometer magnitude signal and adjust PPG frequency estimate.\n",
    "    Args:\n",
    "        acc_magnitude_power: Power spectral density of accelerometer magnitude signal.\n",
    "        ppg_freq: Dominant frequency in PPG spectrum.\n",
    "        ppg_freqs: Frequency array (Hz).\n",
    "        ppg_power: Power spectral density (1D array).\n",
    "\n",
    "    Returns:\n",
    "        Adjusted PPG frequency estimate (float) or None if no valid peak.\n",
    "    \"\"\"\n",
    "    # Find dominant frequency in accelerometer magnitude spectrum\n",
    "    acc_freq = FindDominantFrequency(ppg_freqs, acc_magnitude_power)\n",
    "    if acc_freq is not None and ppg_freq is not None and abs(acc_freq - ppg_freq) < 0.1:\n",
    "        # If they are close, find the next strongest PPG frequency.\n",
    "        # We do this by zeroing out the power at the current PPG frequency and finding the next strongest peak.\n",
    "        ppg_power[ppg_freqs == ppg_freq] = 0\n",
    "        ppg_freq = FindDominantFrequency(ppg_freqs, ppg_power)\n",
    "    return ppg_freq\n",
    "\n",
    "\n",
    "def PlotSpectogram(freqs, times, power, spec, title):\n",
    "    \"\"\"\n",
    "    Plot spectrogram\n",
    "    Args:\n",
    "        freqs: Frequency array (Hz)\n",
    "        times: Time array (s)\n",
    "        power: Power spectral density (1D array)\n",
    "        spec: Spectrogram (2D array of power values)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(freqs, power, color='black')\n",
    "    plt.title(f\"{title} Power Spectrum\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(ymin=58, ymax=62)\n",
    "    plt.imshow(spec, aspect='auto', cmap='magma',\n",
    "               extent=(times[0], times[-1], freqs[0], freqs[-1]))\n",
    "    plt.colorbar(label=\"Power\")\n",
    "    plt.title(f\"{title} Spectrogram\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def ComputeConfidence(ppg, est_bpm, fs, Wn):\n",
    "    \"\"\"\n",
    "    Compute confidence of pulse rate estimate.\n",
    "    Args:\n",
    "        ppg: PPG signal (1D numpy array)\n",
    "        est_bpm: Estimated pulse rate in BPM\n",
    "        fs: Sampling frequency\n",
    "        Wn: Critical frequencies for bandpass filter\n",
    "    Returns:\n",
    "        confidence: Confidence value\n",
    "    \"\"\"\n",
    "    # Bandpass filter ppg signal\n",
    "    ppg = BandpassFilter(ppg, Wn, fs)\n",
    "\n",
    "    # Compute spectrogram for PPG signal\n",
    "    ppg_freqs, _, ppg_power, _ = CreateSpectogram(ppg, fs)\n",
    "\n",
    "    # Compute the confidence by:\n",
    "    # 1. Create a boolean mask for the frequency values that are within 1 Hz of the pulse rate estimate.\n",
    "    # 2. Use the mask to select the spectral power values that are close to the pulse rate estimate.\n",
    "    # 3. Sum the spectral power values from step 2.\n",
    "    # 4. Divide the sum by the total sum of all spectral power values.\n",
    "    # This shows the proportion of the total power that is concentrated close to the estimated pulse rate.\n",
    "    confidence = np.sum(ppg_power[np.abs(ppg_freqs - est_bpm) <= 1]) / np.sum(ppg_power)\n",
    "\n",
    "    return confidence\n",
    "\n",
    "\n",
    "def LowpassFilter(signal, fs):\n",
    "    \"\"\"\n",
    "    Low-pass filter the signal.\n",
    "    Args:\n",
    "        signal: The signal.\n",
    "        param fs: The sampling frequency.\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(3, 12, btype='lowpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "def Featurize(accx, accy, accz, fs):\n",
    "    \"\"\"A partial featurization of the accelerometer signal.\n",
    "\n",
    "    Args:\n",
    "        accx: (np.array) x-channel of the accelerometer.\n",
    "        accy: (np.array) y-channel of the accelerometer.\n",
    "        accz: (np.array) z-channel of the accelerometer.\n",
    "        fs: (number) the sampling rate of the accelerometer\n",
    "\n",
    "    Returns:\n",
    "        n-tuple of accelerometer features\n",
    "    \"\"\"\n",
    "\n",
    "    accx = LowpassFilter(accx, fs)\n",
    "    accy = LowpassFilter(accy, fs)\n",
    "    accz = LowpassFilter(accz, fs)\n",
    "\n",
    "    # The mean of the x-channel\n",
    "    mn_x = np.mean(accx)\n",
    "\n",
    "    # The standard deviation of the x-channel\n",
    "    std_x = np.std(accx)\n",
    "\n",
    "    # The 5th percentile of the x-channel\n",
    "    p5_x = np.percentile(accx, 5)\n",
    "\n",
    "    # The pearson correlation coefficient between the x and y channels\n",
    "    corr_xy = sp.stats.pearsonr(accx, accy)[0]\n",
    "\n",
    "    # The total AC energy of the x-axis\n",
    "    energy_x = np.sum(np.square(accx - np.mean(accx)))\n",
    "\n",
    "    # Take an FFT of the signal. If the signal is too short, 0-pad it so we have at least 2046 points in the FFT.\n",
    "    fft_len = max(len(accx), 2046)\n",
    "\n",
    "    # Create an array of frequency bins\n",
    "    fft_freqs = np.fft.rfftfreq(fft_len, 1 / fs)\n",
    "\n",
    "    # Take an FFT of the centered signal\n",
    "    fft_x = np.fft.rfft(accx - np.mean(accx), fft_len)\n",
    "\n",
    "    # The frequency with the most power between 0.25 and 12 Hz\n",
    "    dominant_frequency_x = fft_freqs[np.argmax(np.abs(fft_x[(fft_freqs >= 0.25) & (fft_freqs <= 12)]))]\n",
    "\n",
    "    # The fraction of energy between 2 and 3 Hz in the x-channel\n",
    "    spectral_energy_x = np.square(np.abs(fft_x))\n",
    "    energy_23_x = np.sum(spectral_energy_x[(fft_freqs >= 2) & (fft_freqs <= 3)]) / np.sum(spectral_energy_x)\n",
    "\n",
    "    return (mn_x,\n",
    "            std_x,\n",
    "            p5_x,\n",
    "            corr_xy,\n",
    "            energy_x,\n",
    "            dominant_frequency_x,\n",
    "            energy_23_x)\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl, fs=125, window_length_s=8, window_shift_s=2, Wn=(40 / 60, 240 / 60),\n",
    "                          peak_detect_threshold=0.5, ppg_quality_threshold=0.5, train=False):\n",
    "    \"\"\"\n",
    "    The algorithm for estimating pulse rate.\n",
    "\n",
    "    This algorithm works by estimating the pulse rate from PPG and accelerometer signals using a machine learning model.\n",
    "    The algorithm begins by loading the data from the data file and the reference data from the reference file. It then\n",
    "    filters the signals using a bandpass filter. The accelerometer signals are combined into a single magnitude signal.\n",
    "\n",
    "    To estimate the pulse rate, the algorithm first bandpass filters the signals and detects peaks in the PPG signal.\n",
    "    It computes pulse-to-pulse intervals and derives the heart rate from these intervals. The algorithm then computes\n",
    "    the spectrogram for both the PPG signal and the accelerometer magnitude signal.\n",
    "\n",
    "    Next, it finds the dominant frequency in the PPG spectrum and adjusts the PPG frequency estimate for movement using\n",
    "    the accelerometer magnitude. A signal quality index for the PPG is computed. The algorithm extracts features from\n",
    "    the accelerometer signals and uses a trained machine learning model to predict the pulse rate based on these\n",
    "    features. The model is selected and tuned using GridSearchCV to find the optimal hyperparameters. If the motion is\n",
    "    low and the time-domain and frequency-domain estimates agree, the algorithm accepts this as the heart rate.\n",
    "\n",
    "    Args:\n",
    "        data_fl: The data file.\n",
    "        ref_fl: The reference file.\n",
    "        fs: Sampling frequency.\n",
    "        window_length_s: Window length in seconds.\n",
    "        window_shift_s: Window shift in seconds.\n",
    "        Wn: Critical frequencies for bandpass filter.\n",
    "        peak_detect_threshold: Minimum distance between peaks (s).\n",
    "        ppg_quality_threshold: Minimum signal quality index for PPG.\n",
    "        train: Whether to train the model.\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    global plotted, debug, predictions, features, model\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "    # Load model if needed\n",
    "    if model is None and not train:\n",
    "        model = load(\"model.joblib\")\n",
    "\n",
    "    # Load reference data\n",
    "    bpm = LoadTroikaRefFile(ref_fl)\n",
    "\n",
    "    # Compute pulse rate estimates and estimation confidence.\n",
    "    window_length = window_length_s * fs\n",
    "    window_shift = window_shift_s * fs\n",
    "    errors, confidence = [], []\n",
    "\n",
    "    for i in range(0, len(ppg) - window_length, window_shift):\n",
    "        # Verify index is within bpm array bounds\n",
    "        bpm_idx = i // (fs * window_shift)\n",
    "        if bpm_idx >= len(bpm):\n",
    "            break\n",
    "\n",
    "        ppg_window = ppg[i:i + fs * window_length]\n",
    "        accx_window = accx[i:i + fs * window_length]\n",
    "        accy_window = accy[i:i + fs * window_length]\n",
    "        accz_window = accz[i:i + fs * window_length]\n",
    "\n",
    "        # Combine accelerometer signals into a single magnitude signal\n",
    "        # This is done by calculating the Euclidean norm of the 3D acceleration vector\n",
    "        # https://en.wikipedia.org/wiki/Euclidean_space#Euclidean_norm\n",
    "        acc_magnitude = np.sqrt(accx_window ** 2 + accy_window ** 2 + accz_window ** 2)\n",
    "\n",
    "        # Get features of window\n",
    "        feats = Featurize(accx_window, accy_window, accz_window, fs)\n",
    "        features.append(feats)\n",
    "\n",
    "        # Get label\n",
    "        label = bpm[bpm_idx]\n",
    "\n",
    "        # Append label\n",
    "        labels.append(label)\n",
    "\n",
    "        if not train:\n",
    "            # Estimate pulse rate\n",
    "            est_bpm = EstimatePulseRate(ppg_window, acc_magnitude, feats, fs, Wn, peak_detect_threshold,\n",
    "                                        ppg_quality_threshold)\n",
    "\n",
    "            # Append prediction\n",
    "            predictions.append(est_bpm)\n",
    "\n",
    "            # Calculate error\n",
    "            errors.append(np.abs(est_bpm - label))\n",
    "\n",
    "            # Compute confidence\n",
    "            confidence.append(ComputeConfidence(ppg_window, est_bpm, fs, Wn))\n",
    "\n",
    "    # Ensure errors and confidence arrays have the same length\n",
    "    min_length = min(len(errors), len(confidence))\n",
    "    errors = errors[:min_length]\n",
    "    confidence = confidence[:min_length]\n",
    "\n",
    "    if debug and not plotted:\n",
    "        # Filter all signals\n",
    "        ppg = BandpassFilter(ppg, Wn, fs)\n",
    "        accx = BandpassFilter(accx, Wn, fs)\n",
    "        accy = BandpassFilter(accy, Wn, fs)\n",
    "        accz = BandpassFilter(accz, Wn, fs)\n",
    "\n",
    "        # Plot spectrograms of entire signals\n",
    "        ppg_freqs, ppg_times, ppg_power, ppg_spec = CreateSpectogram(ppg, fs)\n",
    "        accx_freqs, accx_times, accx_power, accx_spec = CreateSpectogram(accx, fs)\n",
    "        accy_freqs, accy_times, accy_power, accy_spec = CreateSpectogram(accy, fs)\n",
    "        accz_freqs, accz_times, accz_power, accz_spec = CreateSpectogram(accz, fs)\n",
    "\n",
    "        PlotSpectogram(ppg_freqs, ppg_times, ppg_power, ppg_spec, \"PPG\")\n",
    "        PlotSpectogram(accx_freqs, accx_times, accx_power, accx_spec, \"AccX\")\n",
    "        PlotSpectogram(accy_freqs, accy_times, accy_power, accy_spec, \"AccY\")\n",
    "        PlotSpectogram(accz_freqs, accz_times, accz_power, accz_spec, \"AccZ\")\n",
    "        plotted = True\n",
    "\n",
    "    # Return per-estimate mean absolute error and confidence as a 2-tuple of numpy arrays.\n",
    "    errors, confidence = np.array(errors), np.array(confidence)\n",
    "    return errors, confidence.reshape(errors.shape)\n",
    "\n",
    "\n",
    "def GetAndTrainModel():\n",
    "    \"\"\"\n",
    "    Get and train the model.\n",
    "    \"\"\"\n",
    "    global features, labels, model\n",
    "\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        RunPulseRateAlgorithm(data_fl, ref_fl, train=True)\n",
    "\n",
    "    # Define the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Perform Grid Search to find the best model\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error',\n",
    "                               n_jobs=-1)\n",
    "    np_features, np_labels = np.array(features), np.array(labels).flatten()\n",
    "    grid_search.fit(np_features, np_labels)\n",
    "\n",
    "    # Get the best model\n",
    "    model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the cross-validation results\n",
    "    cv_results = grid_search.cv_results_\n",
    "\n",
    "    # Extract and display performance metrics\n",
    "    mean_test_scores = cv_results['mean_test_score']\n",
    "    std_test_scores = cv_results['std_test_score']\n",
    "    params = cv_results['params']\n",
    "\n",
    "    for mean, std, param in zip(mean_test_scores, std_test_scores, params):\n",
    "        print(f\"Mean: {mean}, Std: {std}, Params: {param}\")\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'model.joblib')\n",
    "\n",
    "\n",
    "def InitGlobals():\n",
    "    \"\"\"\n",
    "    Initialize global variables.\n",
    "    \"\"\"\n",
    "    global debug, plotted, predictions, labels, features\n",
    "\n",
    "    debug = False\n",
    "    plotted = False\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "\n",
    "debug = False\n",
    "plotted = False\n",
    "predictions = []\n",
    "labels = []\n",
    "features = []\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
